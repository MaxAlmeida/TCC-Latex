\chapter{Metodologia}
\label{cap:metodologia}
Dado os conceitos que envolvem a diferença de desempenho entre a paravirtualização e a virtualização total e os problemas relacionados a interferência entre máquinas virtuais, neste capítulo serão definidas as abordagens que irão dar norte para o desenvolvimento destre trabalho. Desse modo, em um primeiro momento são apresentados os trabalhos relacionados à análise de desempenho em ambientes virtuais, de modo que é definida a abordagem mais adequada. Por fim, são apresentados as espeficações voltadas para ambientes de testes e coleta de dados.
\section{Trabalhos relacionados}
Essa Seção tem como intuito expor alguns trabalhos relacionados à análise de desempenho. Esses trabalhos também servirão de insumo para desenvolvimento do estudo de interferência de desempenho proposto por este trabalho.

No trabalho apresentado por \cite{koh2007} é feito um estudo de interferência entre aplicações executadas sobre o mesmo \textit{hardware} a partir de duas máquinas virtuais diferentes. Como cargas de trabalho, foram escolhidas aplicações do mundo real utilizadas para compressão, compilação de código fonte, e renderização de \textit{frames}, bem como foram utilizadas ferramentas voltadas para testes de desempenho (\textit{benchmark}).

%\footnotetext[1]{AIM Benchmark (http://sourceforge.net/projects/aimbench)}
%\footnotetext[2]{FreeBench ( http://www.freebench.org/ )}
%\footnotetext[3]{LMbench - Tools for Performance Analysis ( http://www.bitmover.com/lmbench/ )}
%\footnotetext[4] {Bzip (http://www.bzip.org/)}
%\footnotetext[5]{Cachebench memory benchmark (http://icl.cs.utk.edu/projects/llcbench/cachebench.html)}
%\footnotetext[6]{Gzip (http://www.gzip.org/)}
%\footnotetext[7]{IOzone Filesystem Benchmark (http://www.iozone.org)}
%\footnotetext[8]{The Persistence of Vision Raytracer (http://www.povray.org)}

Este trabalho mostra que duas aplicações, cada uma sendo executada em uma máquina virtual diferente, podem interferir no desempenho da outra. A análise de dados é baseada no cálculo de degradação de desempenho de uma aplicação, para o seguinte conjunto de métricas: média de uso de CPU, \textit{cache hits}, \textit{cache misses}, troca de máquinas virtuais por segundo, bloqueio de operações de entrada e saída por segundo, quantidade de requisição de leitura e escrita por segundo e tempo gasto na leitura e escrita para o disco da máquina virtual. %De maneira geral, a observação feita neste trabalho é que determinadas aplicações podem sofrer mais interferências de outras aplicações que possuem o mesmo uso de tipo de recurso. %O exemplo disso é apresentado na Figura \ref{interference_app}, onde uma aplicação executada soz, sem interferência alcança uma pontuação de 1. Duas aplicações que não interferem uma com a outra alcança uma pontuação perto de 2, como \textit{grep}+\textit{povray}. Já executando \textit{grep}+\textit{grep} a pontuação cai para 0.35 %Por exemplo, uma aplicação A que costuma consumir mais CPU, po{de sofrer mais interferência de uma outra aplicação que possui a mesma característica, do que de uma aplicação que realiza operações mais focadas na escrita de disco. Além disso, é proposto por esse trabalho que com resultados desses desempenhos pode se fazer predição de desempenho de uma aplicação qualquer, a partir de analise matemáticas.

%\begin{figure}[!htb]
%\centering
%\includegraphics [keepaspectratio=true,scale=0.65]{figuras/interference_aplications.eps}
%\caption{Variação de desempenho para combinações diferentes de aplicações}
%\cite{koh2007}.
%\label{interference_app}
%\end{figure}

Além disso, é mostrado que com resultados desses desempenhos é possível fazer predição de desempenho de uma aplicação qualquer, a partir de análises estatísticas. Dada a quantidade variáveis, que são as métricas utilizadas para medição de desempenho, as análises estatísticas escolhidas foram a análise de componente principal (PCA) e a análise de regressão linear. Um comparativo entre as análises foi feito de modo que chegou-se a conclusão que análise por PCA apresentava uma porcentagem de erro menor. Esse trabalho demonstrou que a taxa de erro utilizando o modelo PCA se manteve igual, mesmo utilizando outros servidores físicos com configurações diferentes. Entretanto, uma das restrições deste trabalho é que o mesmo fora aplicado em aplicações que estavam sendo executadas em duas máquinas virtuais. Desse modo, como trabalhos futuros é proposta a investigação no uso de mais máquinas virtuais, e aplicação de modelos não lineares para predição de desempenho bem como a adição de novos tipos de métricas que envolvam outras características a nivel de sistema, tal como desempenho de aplicações de rede.

O trabalho de \citeonline{popiolek2012}, disserta sobre a importância de se utilizar métricas nativas (Tabela \ref{metric_tools}) de sistemas  operacionais tais como \textit{Linux} e \textit{Windows} para detecção de gargalos de desempenho. Esse trabalho acaba focando na aferição de operações de entrada e saída, memória e uso de CPU, utilizando ferramentas de \textit{benchmark} para geração de cargas de trabalho. Seus cenários de teste são basicamente variando de uma a seis máquinas virtuais, sendo que o \textit{hypervisor} utilizado é o \textit{KVM}. As análises permitem mostrar a queda de desempenho como um todo quando se tem o aumento do número de máquinas virtuais em execução (Figura \ref{iobound_experiments}). Como trabalho futuros, propõe-se que sejam feitas análise estatísticas afim de comprovar possíveis relações entre as métricas observadas, alem de determinar o limiar que um sistema pode operar sem ter perda significativa no desempenho.

\begin{table}[H]
\centering
\caption{Contadores de desempenho de disco para \textit{Windows} e \textit{Linux} \cite{popiolek2012}}
\label{metric_tools}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lllll}
\cline{1-4}
\multicolumn{1}{|l|}{Windows}                & \multicolumn{2}{l|}{Linux}                                             & \multicolumn{1}{c|}{\multirow{2}{*}{Descricao}}                                                                                                  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Monitor de Desempenho}  & \multicolumn{1}{l|}{iostat}          & \multicolumn{1}{l|}{df}         & \multicolumn{1}{c|}{}                                                                                                                            &  \\ \cline{1-4}
\multicolumn{1}{|l|}{\%Idle Time}            & \multicolumn{1}{l|}{-}               & \multicolumn{1}{c|}{-}          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Porcentagem de tempo\\  que o disco permanece inativo\end{tabular}}                               &  \\ \cline{1-4}
\multicolumn{1}{|l|}{(Disk Bytes/sec)/ 1024} & \multicolumn{1}{l|}{(rKB/s)+(wKB/s)} & \multicolumn{1}{c|}{-}          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Número de Kilobytes \\ lidos/escritos por segundo\end{tabular}}                                   &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Disk Transfers/sec}     & \multicolumn{1}{l|}{(r/s)+(w/s)}     & \multicolumn{1}{c|}{-}          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Número de requisições\\  por segundo completadas\end{tabular}}                                    &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Split IO/sec}           & \multicolumn{1}{c|}{-}               & \multicolumn{1}{c|}{-}          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Número de requisições por segundo\\  que foram divididas\\ em múltiplas requisições\end{tabular}} &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Free Megabytes}         & \multicolumn{1}{c|}{-}               & \multicolumn{1}{l|}{Disponível} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Megabytes disponíveis para uso\\  em unidade de armazenamento\end{tabular}}                       &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Avg. Disk sec/Transfer} & \multicolumn{1}{l|}{Await}           & \multicolumn{1}{c|}{-}          & \multicolumn{1}{l|}{Média de tempo para completar uma requisição}                                                                                &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Avg. DIsk Queue Length} & \multicolumn{1}{l|}{avgqu-sz}        & \multicolumn{1}{c|}{-}          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Média de tamanho de filas de requisições\\  esperando pelo disco rígido\end{tabular}}             &  \\ \cline{1-4}
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  &  \\
                                             &                                      &                                 &                                                                                                                                                  & 
\end{tabular}}
\end{table}

\begin{figure}[!htb]
\centering
\includegraphics [keepaspectratio=true,scale=0.65]{figuras/iobound_experiments.eps}
\caption{Taxa de escrita em disco para experimentos voltados para E/S.}
\cite{popiolek2012}.
\label{iobound_experiments}
\end{figure}

Por fim, o trabalho de \citeonline{huber2011} tem como intuito prover um modelo genérico de predição de desempenho a partir de determinados fatores que podem interferir na virtualização(Figura \ref{influence_factors}). A idéia é que esse modelo genérico seja aplicável em diferentes tipos plataformas de virtualização. Assim, neste trabalho os experimentos são aplicados nos \textit{hypervisors } \textit{Citrix XenServer 5.5} e \textit{VMware ESX 4.0}. Os fatores categorizados para os experimentos foram: tipo de virtualização, configuração de gerenciamento de recursos e perfis de cargas de trabalho. Para o tipo de virtualização, os experimentos consistem em observar a perda de desempenho ocasionada com a virtualização. Na categoria de configuração de gerenciamento de recursos, são considerados fatores de configuração como número de máquinas virtuais e afinidade de núcleo. E para cargas de trabalho, foram executadas algumas ferramentas de \textit{benchmarking} a fim de se analisar diversas cargas de trabalho. Para o modelo de predição, fora utilizada análise de regressão linear assim como feito em \citeonline{koh2007}. 

\begin{figure}[!htb]
\centering
\includegraphics [keepaspectratio=true,scale=0.40]{figuras/factors_influence.eps}
\caption{Fatores que influenciam no desempenho de plataformas virtualizadas}
\cite{huber2011}.
\label{influence_factors}
\end{figure} 

Os trabalhos de \citeonline{koh2007} e \citeonline{huber2011} tem como ponto em comum o desenvolvimento de mecanismos que possam predizer o desempenho em ambientes virtualizados, uma característica interessante é que \citeonline{huber2011} aplica os mesmos experimentos em \textit{hypervisors} com arquiteturas distintas. Já o trabalho de \citeonline{koh2007}, apesar de fazer essa análise em apenas um \textit{hypervisor}, acaba focando na interferência que determinadas aplicações utilizadas no cotidiano, dependendo de sua característica a nivel de sistema, podem ocasionar em outras aplicações executadas em máquinas virtuais diferentes, no mesmo servidor físico,  trazendo assim um experimento mais próximo do que acontece no mundo real, tornando isso um diferencial deste trabalho. Desse modo, ainda com relação ao trabalho de \citeonline{koh2007}, um questionamento que poderia ser feito é com relação a aplicabilidade deste experimento em um outro \textit{hypervisor} com um tipo de virtualização diferente do apresentado no trabalho. Já o trabalho de \citeonline{popiolek2012} é mais voltado para monitoramento, análise de desempenho e detecção de gargalos a partir de métricas nativas do sistema operacional não sendo feito, como nos outros trabalhos, qualquer iniciativa de predição de desempenho.

Dado os trabahos relacionados apresentados, o de \citeonline{koh2007} foi o que apresentou resultados mais significativos em um nivel de detalhamento mais avançado, sendo dessa forma considerado o mais adequado para ser utilizado como insumo para desenvolvimento deste trabalho. Ainda mais, quando se pode verificar a extensibiliidade do trabalho de \citeonline{koh2007} para outro \textit{hypervisor} como o \textit{KVM}. Entretanto, em aspectos metodológicos, o trabaho se demonstrou bastante deficiente, dificultando de certo modo a replicação dos experimentos realizados. A partir disso, uma outra contribuição deste trabalho se comparado ao de \citeonline{koh2007} é o detalhamento dos experimentos efetuados, de modo que o mesmo possa ser replicado em trabalhos futuros.

%\section{Questão problema}
%Dado que o trabalho de \citeonline{koh2007} apresenta uma análise de interferência de desempenho utilizando o \textit{XEN}, que é um %\textit{hypervisor} conhecido por utilizar a paravirtualização para provimento de máquinas virtuais, este trabalho visará responder %a seguinte questão:

%\textit{ Qual  grau de interferência entra máquinas virtuais, executando sob o mesmo servidor, utilizando o \textit{KVM} como \textit{hypervisor}, que implementa a virtualização total?  }
 
%Tendo essa questão como ponto de referência, a proposta deste trabalho é a aplicar o mesmo estudo de interferência entre máquinas /%virtuais feito no trabalho de \citeonline{koh2007}, de modo que sejam comparados grau de interferência entre as duas técnicas de %virtualização existentes: paravirtualização e virtualização total.
%\subsection{Hipótese}
%A partir da questão problema definiu-se a seguinte hipótese que irá motivar a realização deste trabalho:

%\begin{itemize}
    %\item \textit{H1}: O estudo de interferência entre aplicações executadas em ambientes virtualizados proposto por \citeonline{koh2007} é aplicável em outros tipos de plataforma de virtualização 
    
	%\item \textit{H1}: O \textit{XEN}, por utilizar a paravirtualização como técnica de virtualização, apresenta menor interferência entre máquinas virtuais do que o \textit{KVM}. Como explicado no capítulo \ref{sec:infraestrutura}, a paravirtualização tende a possuir um ganho de desempenho se comparado com  virtualização total. Comparando os resultados do trabalho de \citeonline{koh2007}, feitos com \textit{hypervisor} \textit{XEN}(paravirtualização), com os resultados de interferência provenientes de um \textit{hypervisor} \textit{KVM}(Virtualização total), é uma ótima oportunidade de verificar a diferença de desempenho entre os dois tipos de virtualização.
	%\item \textit{H2}: O modelo de prediçã proposto por \citeonline{koh2007} man
	%\item \textit{H2}: O modelo de predição proposto por \citeonline{koh2007} é aplicável em um ambiente com características distintas de \textit{hardware} e de plataforma de virtualização. Em seu trabalho, \citeonline{koh2007} é relatado que esforços já estavam sendo empreendidos para verificar a portabilidade do seu estudo de interferência para outros tipos de ambientes de \textit{hardware}. Entretanto, não foi encontrado qualquer tipo de evidência em algum trabalho futuro que apresente a aplicação deste estudo em outro tipo de ambiente. Dessa forma, com outro tipo de \textit{hypervisor} (\textit{KVM}) e com configurações de \textit{hardware} apresentados no capítulo \ref{cap:resultados}, é interessante verificar aplicabilidade deste experimento para outros tipo de plataforma tanto no quesito de \textit{hardware} quanto de virtualização. 
	%\item \textit{H3}: Com esse mecanismo de predição de desempenho é possível propor uma nova distribuição de aplicações em um ambiente com múltiplas máquinas virtuais.  %reformular
%\end{itemize}
	
\section{Ambiente de Testes}\label{sec:ambiente_teste}
Para o estudo proposto por este trabalho foi definido um  ambiente de testes que consiste no uso de máquinas virtuais com aplicações voltadas para realização
de testes de \textit{benchmark}. Tais aplicações foram definidas a partir do trabalho de \citeonline{koh2007}, tendo essas sido escolhidas visando o estresse de vários aspectos de sistema e de \textit{hardware}. Com a infraestrutura de computação em nuvem implementada com o \textit{OpenNebula} foi possível a criação desses ambientes de testes em máquinas virtuais criadas remotamente. As máquinas virtuais possuíam, como configuração, sistema operacional \textit{Centos 7}, espaço em disco de 15GB e 1GB de memória \textit{RAM}. Em um primeiro momento as aplicações foram instaladas e testadas de modo que se pudesse observar quais são os comandos utilizados para funcionamento das mesmas, em seguida foram criados \textit{snapshots} das máquinas virtuais com o auxílio provido pelo \textit{OpenNebula}. 

Assim era possível criar, destruir e efetuar quaisquer tipos de testes com relativa comodidade. Entre as aplicações escolhidas estão típicas provedoras de \textit{stress} computacional no cotidiano, tais como compilação de código fonte, compressão e criptação de arquivos e rendenrização de \textit{frames}. Há também ferramentas voltadas para geração de testes de \textit{benchmark} tais como \textit{Cachebench} e \textit{AIM Benchmark suite}. A seguir é feita uma breve descrição das ferramentas utilizadas.

\begin{itemize}
\item \textit{Add\_double} \footnotemark[3]                                                                                                                               é um dos vários programas de testes de carga existentes no \textit{AIM benchmark suite}. É responsável por medir operações de adição de dupla precisão.

\item \textit{Bzip2} \footnotemark[4] e \textit{Gzip} \footnotemark[5] são aplicações típicas para compressão e descompressão de arquivos. Com uso de arquivos grandes é possível gerar cargas de trabalhos usando essas ferramentas.

\item \textit{Ccrypt} \footnotemark[6] é uma ferramenta de código aberto voltada para encriptação e desencriptação de arquivos. Foi desenvolvido com o intuito de substituir a aplicação padrão do \textit{unix}, o \textit{crypt}.

\item \textit{Cachebench} \footnotemark[7] é uma ferramenta de \textit{benchmark} de código aberto desenvolvida para avaliar o desempempenho do subsistema de memória. Atualmente é integrado \textit{LLCbench} (\textit{Low-Level Characterization Benchmarks}).

\item \textit{Cat} e \textit{Grep} são comandos padrões em sistemas \textit{Linux} que são responsáveis por gerar requisições de leitura no disco. \textit{Cat} é reponsável por mostrar conteúdo de arquivos bem como combina-los e criar outros novos. Enquanto que \textit{Grep} é utilizado para busca de palavras em arquivos texto.

\item \textit{cp} e \textit{dd} são outros comandos padrões em sistemas \textit{Linux}, neste caso, responsáveis por gerar atividades voltadas para escrita de disco. \textit{cp} é utilizado para copiar arquivos e diretórios. O comando \textit{dd} é utilizado para criação de imagens e cópias de arquivos.

\item \textit{Iozone} \footnotemark[8] é uma ferramenta de \textit{benchmark} utilizada, voltada para testes de operações de disco, tais como leitura e escrita.

\item \textit{Make} é um comando nativo em sistemas operacionais \textit{Linux}, responsável por automatizar um conjunto de procedimentos, principalmente  a compilação de programas grandes que possuem vários arquivos com códigos fontes. 

\item \textit{Povray} \footnotemark[9] é uma ferramenta de codigo aberto voltada para para renderização de quadros com gráficos 3-D.

\end{itemize}

\footnotetext[3]{AIM Benchmark (http://sourceforge.net/projects/aimbench)}
\footnotetext[4] {Bzip2 (http://www.bzip.org/)}
\footnotetext[5]{Gzip (http://www.gzip.org/)}
\footnotetext[6]{Ccrypt (http://ccrypt.sourceforge.net/}
\footnotetext[7]{Cachebench memory benchmark (http://icl.cs.utk.edu/projects/llcbench/cachebench.html)}
\footnotetext[8]{IOzone Filesystem Benchmark (http://www.iozone.org)}
\footnotetext[9]{The Persistence of Vision Raytracer (http://www.povray.org)}

%Além das ferramentas já mostradas outras aplicações foram utilizadas no trabalho de \citeonline{koh2007}: \textit{Spinlock}, \textit{CacheBuster} e \textit{Analyser}.

\section{Coleta de dados} 
Dada a configuração do servidor apresentado no Apêndice \ref{sec:infraestrutura}, um dos receios era de que os testes de \textit{benchmark} feitos não fossem suficientes para apresentarem interferência entre as máquinas virtuais, desse modo optou-se por dividr a coleta de dados em três experimentos práticos de modo que, nos dois primeiros experimentos fossem verificadas as interferências entre os diversos tipos de aplicações. Assim, para cada experimento foi definido objetivos específicos a serem alcançados.
%A coleta de dados foi efetuada a apartir da realização de três experimentos práticos. Para cada experimento foi definido objetivos específicos a serem alcançados.%

O primeiro experimento tinha como objetivo verificar a existência da interferência entre máquinas virtuais no servidor físico utilizado, sendo neste experimento executado com poucas ferramentas dado que os resultados eram mais voltados para verificação da interferência e motivação da continuidade do trabalho em si do que para uma análise mais profunda. No segundo experimento, teve como objetivo verificar a variação de interferência para o conjunto completo das ferramentas já apresentadas sendo construída uma matriz \textit{n x n} com todas as possibilidades. No terceiro experimento, teve como foco a avaliação da interferência para diversos tipos de caracteristicas a níveis de sistema tais como média de utilização de \textit{CPU} e \textit{leitura e escrita de disco por segundo}, por exemplo. Sendo os resultados deste terceiro e último experimentos utilizados como insumo para análise de dados apresentada neste trabalho.% A apresentação dos dados referentes a cada experimento foi baseada nos resultados apresentados no trabalho de \citeonline{koh2007}, visando assim um comparativo dos resultados encontrados para os dois diferentes tipos de virtualizaçaõ: paravirtualização e virtualização total.

\subsection{Cálculo da Interferência}
Os procedimentos adotados para o cálculo da interferência seguem os propostos no trabalho de \citeonline{koh2007}. Desse modo, duas máquinas virtuais, com as especificações e aplicações de \textit{benchmark} apresentadas na Seção \ref{sec:ambiente_teste}, são criadas em um servidor utilizando \textit{hypervisor} \textit{KVM}. Cada máquina virtual, denominadas \textit{'dom1'} e \textit{'dom2'} respectivamente, executa uma das aplicações de \textit{benchmarking}. 

Uma aplicação executando em \textit{dom1} é chamada de aplicação \textit{foreground}, e a que estiver executando em \textit{dom2} é a aplicação \textit{background}. Por questões de notação uma aplicação \textit{foreground} executando contra uma aplicação \textit{background} é denotada como F@B. Um dos procedimentos adotados é garantir que a aplicação \textit{background} mantenha sua execução até que a aplicação \textit{foreground} termine. Para o segundo e terceiro experimento cada aplicação é executada de modo que seja tanto \textit{background} quanto \textit{foreground}, sendo construída dessa forma uma matrix \textit{n x n} com todos os possíveis resultados. %Destaca-se os dados referentes a interferência são mensurados em uma aplicação \textit{foreground}.

A fim de observar o quanto o desempenho é afetado pela interferência gerada por uma aplicação executada em outra máquina virtual, é feita a medida da degradação a partir do desempenho padrão de uma aplicação, essa medida denomina-se pontuação normalizada. Assim, para calcular a pontuação normalizada de uma aplicação, primeiro é definida a pontuação de desempenho inativa que é a pontuação de uma aplicação quando executada contra uma máquina virtual inativa, ou seja sem nenhuma aplicação executando. Então, em seguida é feito o cálculo da pontuação normalizada de uma aplicação F contra B, dividindo a pontuação de desempenho de F contra B pela pontuação de desempenho inativa de F. Assim define-se NS(F@B), como sendo a pontuação normalizada de F contra B,

\begin{equation}
\label{eq:degradation}
NS(F@B) = PontuaçãoDesempenho(F@B)/PontuaçãoDesempenho(F@Inativo)
\end{equation}

A partir disso, é feito cálculo do desempenho combinado de duas aplicações, F e B, em cada máquina virtual.

\begin{equation}
\label{eq:combined}
NS ( F + B ) = NS ( F @ B ) + NS ( B @ F )
\end{equation}
Sendo NS(F@B) e  NS(B@F) medidos em dois testes separados. Para medida de desempenho, nos dois primeiros primeiros experimentos são utilizadas as pontuações geradas pelas próprias aplicações. Entretanto, algumas aplicações, aquelas que não são voltadas para \textit{benchmark}, não geram pontuações explícitas. Para essas aplicações, fora definido como o inverso do tempo necessário para sua execuação, como pontuação de desempenho. Na tabela \ref{table-aplications} é apresentado o maior recurso utilizado bem como a medida de desempenho utilizada para cada ferramenta. Para o terceiro experimento as pontuações utilizadas são métricas de desempenho a nível de sistema.

\begin{table}[!htb]
\centering
\caption{Aplicações utilizadas para geração de cargas e trabalho}
\label{table-aplications}
\begin{tabular}{|l|c|c|}
\hline
Nome        & \multicolumn{1}{l|}{Maior Recurso Utilizado} & \multicolumn{1}{l|}{Medida de Desempenho} \\ \hline
Add\_double & CPU                                          & Pontuação                                  \\ \hline
Bzip2       & Misto                                        & Tempo                                      \\ \hline
Cat         & Disco                                        & Tempo                                      \\ \hline
Cachebench  & Memória                                      & Pontuação                                  \\ \hline
Ccrypt      & Misto                                        & Tempo                                      \\ \hline
Cp          & Disco                                        & Tempo                                      \\ \hline
Dd          & Disco                                        & Tempo                                      \\ \hline
Grep        & Disco                                        & Tempo                                      \\ \hline
Gzip        & Misto                                        & Tempo                                      \\ \hline
Iozone      & Disco                                        & Pontuação                                  \\ \hline
Make        & Misto                                        & Tempo                                      \\ \hline
Povray      & Misto                                        & Tempo                                      \\ \hline
\end{tabular}
\end{table}

Dessa forma, uma pontuação normalizada(F@B) próxima ou igual a 1 é um resultado que indica que o desempenho de uma aplicação \textit{F} contra \textit{B} sofreu baixa degradação.

%Para observar o grau de interferência ocasionada por uma aplicação que está sendo executada em outra máquina virutal, é feito um cálculo  de degradação a partir da pontuação de desempe
\subsection{Métricas de desempenho a nível de sistema}
No trabalho de \citeonline{koh2007} são utilizadas métricas de desempenho a nível de sistema ( denominadas \textit{System-level Workload Characteristics}) de forma a analisar melhor o grau de interferência em diversos aspectos de sistema na máquina virtual. Segundo \citeonline{koh2007}, o fato desse tipo de métrica de desempenho ser independente de qualquer tipo de microarquitetura subjacente, garante que seja possível ser feitas comparações através dos diferentes tipos de servidores físicos.

No trabalho de \citeonline{koh2007} essas métricas são obtidas através de um \textit{hypervisor} instrumentalizado, de modo que elas são coletadas de fora da máquina virtual, possibilitando assim que a interferência da coleta seja praticamente nula. A partir disso, fora feita uma investigação de como o \textit{hypervisor} utilizado, o \textit{KVM}, ou a plataforma em nuvem \textit{OpenNebula} poderia prover essas métricas sem que fosse necessário qualquer tipo de interferência na máquina virtual para coleta desses dados.

Para o \textit{KVM} chegou-se a ferramentas como o \textit{iperf-kvm} e \textit{kvm-stat}. Entretanto, as informações apresentadas por essas ferramentas não eram claras e, para o caso do \textit{kvm-stat}, não detalhava por máquina virtual e sim para  \textit{hypervisor} inteiro. Mesmo no \textit{OpenNebula} as informações apresentadas consistiam em uso de espaço em disco, memória utilizada e quantidade de \textit{CPU} alocado por máquina virtual, não sendo essas informações, relevantes para o estudo proposto. Assim, a abordagem escolhida foi o uso de uma ferramentas típica para monitoramento de desempenho: \textit{iostat} para operações de disco e o \textit{mpstat} para \textit{CPU}. %o \textit{Munin} em conjunto com o \textit{Cachegrind} e o \textit{Virsh}. A seguir são apresentadas as métricas coletadas neste trabalho.

\begin{itemize}
\item \textbf{Média de CPU}(\textit{cpuutil}): É calculado dividindo o \textit{tempo de CPU} alcançado por uma máquina virtual para executar uma aplicação, pelo tempo de execução dessa aplicação. O tempo de CPU é coletado pelo \textit{libvirt}.

\item \textbf{Requisições de escrita e leitura de disco por segundo }(\textit{writes\_issued, reads\_issued}) e \textbf{Tempo gasto para escrita e leitura do disco}(\textit{time\_writing, time\_reading}): Quantidade de requisições no disco e o tempo para operações de escrita e leitura são bons indicadores de operações de entrada e saída. Essa valores são coletados utilizando o \textit{Munin}.

\end{itemize}

\subsection{Procedimentos experimentais}
Um dos impasses encontrados no desenvolvimento deste trabalho foi executar os mesmos procedimentos feitos por \citeonline{koh2007}, dado que não fica tão explícito quais foram os caminhos adotados para execução de cada aplicação ou ferramentas de \textit{benchmark}. Dessa maneira, fora necessário definir experimentalmente os procedimentos a serem adotados para geração estress computacional e coleta de dados. De maneira geral, cada ferramenta era executada 15 vezes, em cada execução o tempo de execução ou a pontuação alcançada(para ferramentas que tinha pontuações explícitas) era coletado. Alcançada a quantidade de 15 execuções era calculada uma média dos dados coletados.

Inicialmente foi definido um número de 30 execuções por ferramentas(uma amostragem de 30 valores). Entretanto, principalmente no segundo experimento, a estimativa de tempo para execução de todas ferramentas se mostrou demasiadamente grande. Dessa forma, cogitou-se o uso de 10 ou 15 valores. De modo a chegar a um valor adequado para amostragem, levando se conta a quantidade o tempo necesário para essa quantidade. Calculou-se o desvio padrão das ferramentas \textit{Make} e \textit{Bzip} para 10, 15 e 30 valores de amostragem, a fim de verificar a diferença de dispersão entre esses valores. A tabela \ref{desvio} apresenta esses resultados.     

\begin{table}[!h]
\centering
\caption{Desvio Padrão para diferentes quantidades de valores para \textit{Povray} e \textit{Make}}
\label{desvio}
\begin{tabular}{|l|c|l|l|}
\hline
Ferramenta            & \multicolumn{1}{l|}{Quantidade de Valores} & Desvio Padrão  & Média         \\ \hline
\multirow{3}{*}{Make} & 10                                         & 0.993032191489 & 44.5126       \\ \cline{2-4} 
                      & 15                                         & 0.436379367501 & 44.3846666667 \\ \cline{2-4} 
                      & 30                                         & 0.379019873315 & 44.3230666667 \\ \hline
\multirow{3}{*}{Gzip} & 10                                         & 0.394128701484 & 31.9069       \\ \cline{2-4} 
                      & 15                                         & 0.229156361673 & 32.0740666667 \\ \cline{2-4} 
                      & 30                                         & 0.249032463378 & 32.1289333333 \\ \hline
\end{tabular}
\end{table}

A diferença do desvio padrão para uma amostragem entre 15 e 30 valores foi considerada irrelevante se comparada com a média alcançada dos resultados. O desvio padrão para uma amostragem de 10 valores mostrou uma dispersão mais elevada, mesmo assim ainda irrelevante a diferença se comparada com a média, mas visando uma margem mas segura de erro, acabou-se definido o uso de uma amostragem de 15 valores. 

Para geração de carga de trabalho a partir das ferramentas selecionadas, alguns procedimentos foram definidos empiricamente, principalmente paras aplicações utilizadas no cotidiano. A seguir são descritos alguns procedimentos adotados para cada ferramenta selecionada.

Para a coleta de dados referentes ás métricas de desempenho a nível de sistema, foi definido uma coleta de 300 valores por cada métrica. 
\begin{itemize}
\item \textit{Add\_double}: A única opção para execução dessa ferramenta de \textit{Benchmarking} é a quantidade de tempo que é executada. Foi escolhido um tempo de 30 segundos.

\item \textit{Bzip2, cp, cat, gzip}: Para essas ferramentas foi definido o uso de um arquivo com tamanho de 3 \textit{GB} ( o triplo da memoria da máquina virtual) de modo que se evitasse o efeito de \textit{cache} pela memória. Desse modo, os comandos \textit{cat} e \textit{cp} geravam cargas a partir da geração de cópias desse arquivo. Aplicações \textit{Bzip2} e \textit{gzip}, efetuavam a compreensão desse arquivo com a opção \textit{--best}.

\item \textit{Cachebench}: Possui diferentes tipos de testes de \textit{benchmark} para o sistema de memória, sendo executados para tamanhos variáveis de alocação de memória. É executado com as opções: \textit{-b} ( \textit{Read/Modify/Write benchmark}), \textit{-x0}, \textit{-m24}, \textit{-d1}( 1 segundo por iteração), \textit{-e1}(1 repetição do teste por tamanho de alocação da memória) . Como resultado é considerado apenas o valor da taxa de transferência em \textit{MB/s} apresentado para o maior tamanho de alocação de memória que a ferramenta suporta.

\item  \textit{Ccrypt, grep}: Para essas ferramentas foi utilizado um arquivo de 2\textit{GB}, gerado com números \textit{randômicos} a parir de \textit{/dev/random}. Com \textit{Ccrypt} é efetuada a encriptação do arquivo, com o \textit{grep} é feita uma busca pelo número \textit{'123'}.

\item \textit{Iozone}: Essa ferramenta foi executada de modo que se efetuasse os testes de leitura e escrita no modo sequencial e rândomico.

\item \textit{Make}: Para geração de carga foi efetuada a compilação do código fonte da versão mais recente do \textit{apache} \footnotemark[8]                                                                                                                              . Coletado o tempo para execução da compilação, a mesma é desfeita para que em seguida seja executada novamente, obtendo assim a quantidade de valores necessária para o cálculo da média.

\item \textit{Povray}: Possui um módulo próprio para execução de testes de \textit{benchmark}. O próprio módulo de \textit{benchmark} já contabiliza o tempo de execução dos ceńarios de renderização de \textit{frame} da ferramenta. 

\end{itemize}

Para coleta de dados de dados foi construídos \textit{scripts}, utilizando a linguagem \textit{Python} \footnotemark[9]                                                                                                                               com a quantidade de execução bem como os comandos utilizados para cada aplicação e ferramenta de \textit{benchmark}, de modo a garantir mais comodidade na obtenção dos dados. Esses \textit{scripts} estão disponíveis via repositório remoto \footnotemark[10].


\footnotetext[8]{http://www.apache.org/}
\footnotetext[9]{https://www.python.org/}
\footnotetext[10]{https://github.com/MaxAlmeida/TCC-SCRIPTS-MONITORING}

